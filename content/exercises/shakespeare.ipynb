{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Shakespeare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise is an introduciton to natural language processing using python. We will be analyzing the most frequently appearing words in one of the books found from [project Gutenberg](https://www.gutenberg.org/) â€” Shakespeare's Hamlet.\n",
    "\n",
    "We will be using a python library called [nltk](https://www.nltk.org/) (Natural language toolkit), which has some really useful features when it comes to learning natural language processing. For this exercise, we need to choose a [corpus](https://en.wikipedia.org/wiki/Text_corpus), which is essentially a language resource that consists of a collection of texts. For this exercise, we will use texts from project Gutenberg. Luckily the nltk-library can provide us with a collection of texts from project Gutenberg. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin to explore the world of natural language processing by browsing the project Gutenberg's book collection available on nltk.corpus -library. For this, we need to import gutenberg-package from the mentioned library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "# Print a list of available books from project Gutenberg\n",
    "gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see there are quite a few books in our corpus. Let's pick 'shakespeare-hamlet.txt', for further analysis.\n",
    "\n",
    "We can use words()-, sents()- and raw()-methods from the gutenberg package to get the words, sentences and raw text from our book. More information from gutenberg corpus package can be founds [here](https://www.nltk.org/book/ch02.html#gutenberg-corpus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = gutenberg.words('shakespeare-hamlet.txt')  # Get words \n",
    "sentences = gutenberg.sents('shakespeare-hamlet.txt') # Get sentences\n",
    "sample = gutenberg.raw('shakespeare-hamlet.txt')  # Get raw text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what is inside those variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "\n",
    "print(\"Words:\", words)\n",
    "print()\n",
    "print(\"Sentences:\", sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Uncomment the following line to read Hamlet\n",
    "#print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, try playing around with each variables we defined so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the final 50 words\n",
    "words[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many words do we have ?\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the final 10 sentences\n",
    "sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the 16th sentence (remember count starts from 0).\n",
    "# Feel free to check any sentence. \n",
    "# You can check multiple sentences as well if you specify start and end sentence.\n",
    "\n",
    "sentences[15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to efficiently analyze the texts, we need to clean the text from any extra characters etc. We can observe the appearence of our raw text word by word by tokenizing it. Tokenization is essentially splitting a phrase, sentence, paragraph, or an entire text document into smaller units, such as individual words or terms, sentences, etc. Each of these smaller units are called tokens. We can perform tokenization to words by [splitting](https://docs.python.org/3.3/library/stdtypes.html#str.split) it on each whitespace or line break character.\n",
    "\n",
    "See the appearence of the text on the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_split = sample.split() # Split the text \n",
    "sample_split[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are still quite a few unnecessary characters, such as square brackets or dots. Also, we would like to make all characters lowercase to be able to analyze them more easily.\n",
    "\n",
    "Manipulating the sample is efficient with a python library called [regex](https://docs.python.org/3/library/re.html). See a detailed explanation for cleaning the sample text using regex on the next code cell. In this exercise we will use [`sub()`](https://docs.python.org/3/library/re.html#re.sub)-function from regex-library to replace characters in a string. For examples using regex, see [this](https://www.w3schools.com/python/python_regex.asp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import re-library (regex)\n",
    "import re\n",
    "\n",
    "# Initialize an empty list for cleaned words\n",
    "sample_cleaned = []\n",
    "\n",
    "# Go through each word one by one\n",
    "for word in sample_split: \n",
    "    \n",
    "    # Make the word lowercase\n",
    "    word_lowered = word.lower() \n",
    "    \n",
    "    # This expression will replace all other characters except letters from a-z and numbers with an empty string.\n",
    "    word_cleaned = re.sub(r'[^a-z0-9]+', '', word_lowered) \n",
    "\n",
    "    # Let's make sure that we do not add any empty word strings to our cleaned sample\n",
    "    if word_cleaned != '': \n",
    "        \n",
    "        # Add the cleaned word to cleaned sample list\n",
    "        sample_cleaned.append(word_cleaned)\n",
    "\n",
    "# Print the total number of words and the first 20 words\n",
    "print(len(sample_cleaned))\n",
    "sample_cleaned[0:50] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the list of words start to look better. However, typically we are not interseted in the most common words in the language and we'd like to filter those out from the list. These kind of words are called [stop words](https://en.wikipedia.org/wiki/Stop_word). We could either list the stop words ourselves but we can also use the nltk-library to define the most common stop words in English for us.\n",
    "\n",
    "Let's take a look what kind of stop words are in the nltk.corpus-package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see these are pretty common words that appear in English language. Let's filter these out from our sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an empty list which will be populated with words that are not stop words.\n",
    "sample_nostopwords = []\n",
    "\n",
    "# Go through each word in our cleaned sample\n",
    "for word in sample_cleaned:\n",
    "    \n",
    "    # Only perform the following code if the word is not found in stop_words list.\n",
    "    if word not in stop_words:\n",
    "        \n",
    "        # Add the word to the sample_nostopwords-list.\n",
    "        sample_nostopwords.append(word)\n",
    "\n",
    "sample_nostopwords[:20]\n",
    "myset = set(sample_nostopwords)\n",
    "len(myset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a list of words in our sample text that are cleaned and the stop words are removed. Check how many words are left in our sample_nostopwords -list to see how many words were removed in the previous cell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the length of sample_nostopwords -list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "According to [Wikipedia](https://en.wikipedia.org/wiki/Lemmatisation), lemmatisation in linguistics is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form.\n",
    "\n",
    "Let's do lemmatization for our sample text to continue with our analysis. For this, we can use [`WordNetLemmatizer`](http://www.nltk.org/api/nltk.stem.html#nltk.stem.wordnet.WordNetLemmatizer) from the nltk.stem -package. See how to use it in the following cell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Define an empty list which will be populated with our lemmatized set of words\n",
    "sample_lemmatized = []\n",
    "\n",
    "# Initialize our lemmatizer object. It can be later referred to as wnl.\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "# Go through each word in our sample text\n",
    "for word in sample_nostopwords:\n",
    "    \n",
    "    # Lemmatize the word\n",
    "    word_lemmatized = wnl.lemmatize(word)\n",
    "    \n",
    "    # Add the lemmatized word to the list\n",
    "    sample_lemmatized.append(word_lemmatized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the sample text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally after cleaning and lemmatizing the sample, it is in the desired state. After the lemmatization, our list of words should contain multiple occurences of the same words. Next, we can analyze the sample further to see which words appear most frequently. For this, we will use [FreqDist()](http://www.nltk.org/api/nltk.html#nltk.probability.FreqDist)-class from nltk.probability -package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Make a frequency distribution object from our lemmatized words.\n",
    "freq_dist = FreqDist(sample_lemmatized)\n",
    "\n",
    "# Print information from the frequency distribution\n",
    "print(freq_dist)\n",
    "\n",
    "# By using the \"most_common\"-method of the distribution object, we can print the 20 most common words\n",
    "# Now we can understand Shakespeare's vocabulary set ;)\n",
    "freq_dist.most_common(20)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also plot the distribution using `plot()`-method of the frequency distribution object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the frequency distribution of the 20 most common words\n",
    "freq_dist.plot(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another interesting thing is make a lexical dispersion plot that plots the occurences of a word and how many words from the beginning it appears. To make the plot, first we need to turn our list of words into [nltk.Text](https://www.nltk.org/api/nltk.html#nltk.text.Text)-object and then use [`dispersion_plot()`](https://www.nltk.org/api/nltk.html#nltk.text.Text.dispersion_plot)-method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import Text\n",
    "\n",
    "# Make a Text-object from our sample\n",
    "disp_sample = Text(sample_lemmatized)\n",
    "\n",
    "# Make the dispersion plot with a few words.\n",
    "disp_sample.dispersion_plot(['ham','king','shall','bar','fran'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! We've managed to find out the most commonly used words by Shakespeare in Hamlet and learned how to check that how the words are distributed on the sample.\n",
    "\n",
    "Can you figure out why the word \"ham\" appears to be the most frequently used word when stop words are removed? What are \"bar\" and \"Fran\" on the dispersion plot and why they appear only at the beginning of the book?\n",
    "\n",
    "Try to find out the most common words for some other book yourself (from project Gutenberg, for example)!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
